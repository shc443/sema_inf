{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce353533-804f-4618-ae21-8bfb7c67ac88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VOC_TopicLabeler, VOC_DataModule\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForMaskedLM, DebertaV2Config, DebertaV2ForSequenceClassification, AutoConfig\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mL\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/sema/sema.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from sema import VOC_TopicLabeler, VOC_DataModule\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DebertaV2Config, DebertaV2ForSequenceClassification, AutoConfig\n",
    "import lightning as L\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "from konlpy.tag import Kkma\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "kkma = Kkma()\n",
    "\n",
    "def findall_vec(key,voc):\n",
    "  try:\n",
    "    return re.findall(key, voc)[0]\n",
    "  except:\n",
    "    return ''\n",
    "\n",
    "def findall_vec2(df):\n",
    "  return findall_vec(df['keyword'],df['VOC'])\n",
    "\n",
    "def filter_etc(df):\n",
    "  filt0 = (df['VOC'].str.strip().str.len() < 4).astype(int)\n",
    "  filt1 = df['VOC'].apply(lambda x : bool(re.match(r'^[_\\W]+$', str(x).replace(' ','')))).astype(int)\n",
    "  filt2 = df['VOC'].apply(lambda x : bool(re.match(r'[\\d/-]+$', str(x).replace(' ','')))).astype(int)\n",
    "  filt3 = (df.VOC.str.replace(' ','').str.split('').apply(set).str.len() == 2)\n",
    "  voc_tok = df.VOC.progress_apply(lambda x : Counter(kkma.morphs(x)))\n",
    "  filt4 = voc_tok.isin(voc_etc).astype(int)\n",
    "  return filt0+filt1+filt2+filt3+filt4\n",
    "  #return filt1,filt2,filt3,filt4\n",
    "\n",
    "def kkmaCounter(txt):\n",
    "    try:\n",
    "        return Counter(kkma.morphs(txt))\n",
    "    except: \n",
    "        return txt\n",
    "\n",
    "# Function to remove non-English and non-Korean characters\n",
    "def remove_non_english_korean(string):\n",
    "    # Regular expression to match English and Korean characters\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\uac00-\\ud7a3\\s]', flags=re.UNICODE)\n",
    "    return pattern.sub('', string)\n",
    "    \n",
    "def strip_e(st):\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    return RE_EMOJI.sub(r'', st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e29e06-9ec3-442f-bd7c-fc202782319c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 35\n",
    "BATCH_SIZE = 12\n",
    "MAX_LEN = 256\n",
    "LR = 2e-05\n",
    "opt_thresh=0.5\n",
    "\n",
    "input_file = pd.Series(os.listdir('./data/input'))\n",
    "input_file = input_file[input_file.str.contains('.xlsx')]\n",
    "\n",
    "output_list = pd.Series(os.listdir('./data/output'))\n",
    "running_file = input_file[~input_file.str[:-5].isin(output_list.str.split('_output').str[0])]\n",
    "\n",
    "config = AutoConfig.from_pretrained('team-lucid/deberta-v3-xlarge-korean', output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('team-lucid/deberta-v3-xlarge-korean')\n",
    "\n",
    "with open('data2.pkl', 'rb') as f:\n",
    "    mlb = pickle.load(f)\n",
    "\n",
    "LABEL_COLUMNS = mlb.classes_[:]\n",
    "voc_etc = pd.read_pickle('voc_etc.pkl')\n",
    "keyword = pd.read_pickle('keyword_doc.pkl')\n",
    "\n",
    "new_model = VOC_TopicLabeler.load_from_checkpoint(checkpoint_path=\"team-lucid/deberta-v3-xlarge-korean\" + \"_20ep_full_mar17_dropna.ckpt\", n_classes=18, model = 'team-lucid/deberta-v3-xlarge-korean').cuda()\n",
    "new_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9084143-e92c-4c6e-b2ad-9d2490fd393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.classes_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d06b1f4-4fd3-4d78-80cf-b2dbb8a7f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06781d6c-bb49-45c4-8510-d9940c1f23e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading : 세종충남대학교병원_VOC_20250616.xlsx\n",
      "Filtering : 세종충남대학교병원_VOC_20250616.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2087/2087 [02:12<00:00, 15.74it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting : 세종충남대학교병원_VOC_20250616.xlsx\n",
      "Inferencing : 세종충남대학교병원_VOC_20250616.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9f6f04ca9f4868aa2fcc8cb2b521c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Keywords : 세종충남대학교병원_VOC_20250616.xlsx\n",
      "Saving output File : 세종충남대학교병원_VOC_20250616.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-62238184/ipykernel_3382225/1672501759.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  voc_testset.topic.fillna('기타',inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading : 충남대학교병원_VOC_input_20250618.xlsx\n",
      "Filtering : 충남대학교병원_VOC_input_20250618.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3388/3388 [02:53<00:00, 19.52it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting : 충남대학교병원_VOC_input_20250618.xlsx\n",
      "Inferencing : 충남대학교병원_VOC_input_20250618.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d1edf70fe0427d8e33878a408a5d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Keywords : 충남대학교병원_VOC_input_20250618.xlsx\n",
      "Saving output File : 충남대학교병원_VOC_input_20250618.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-62238184/ipykernel_3382225/1672501759.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  voc_testset.topic.fillna('기타',inplace=True)\n"
     ]
    }
   ],
   "source": [
    "for file in running_file:\n",
    "    #Reading\n",
    "    print('Reading : ' + file)\n",
    "    voc_testset = pd.read_excel(\"data/input/\"+file, dtype=str)\n",
    "    voc = pd.concat([voc_testset.VOC1,voc_testset.VOC2]).sort_index().values\n",
    "    voc_testset = pd.concat([voc_testset]*2).sort_index().iloc[:,1:-2]\n",
    "    voc_testset['VOC'] = voc\n",
    "    voc_testset = voc_testset.dropna(subset='VOC')\n",
    "    voc_testset.reset_index(inplace=True)\n",
    "    voc_testset['label'] = pd.DataFrame(np.zeros((18,voc_testset.shape[0])).T).astype(int).apply(list, axis=1)\n",
    "    \n",
    "    #Filtering\n",
    "    print('Filtering : ' + file)\n",
    "    voc_testset = voc_testset[voc_testset['VOC'] != 'nan']\n",
    "    voc_testset['VOC'] = voc_testset['VOC'].apply(remove_non_english_korean)\n",
    "    voc_testset['VOC'] = voc_testset['VOC'].apply(strip_e)\n",
    "    voc_testset['VOC'] = voc_testset['VOC'].replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    filt0 = (voc_testset['VOC'].str.strip().str.len() < 4).astype(int)\n",
    "    filt1 = voc_testset['VOC'].apply(lambda x : bool(re.match(r'^[_\\W]+$', str(x).replace(' ','')))).astype(int)\n",
    "    filt2 = voc_testset['VOC'].apply(lambda x : bool(re.match(r'[\\d/-]+$', str(x).replace(' ','')))).astype(int)\n",
    "    filt3 = (voc_testset.VOC.str.replace(' ','').str.split('').apply(set).str.len() == 2)\n",
    "    voc_testset = voc_testset[(filt0 + filt1 + filt2 + filt3) == 0]\n",
    "    # voc_testset['VOC'].progress_apply(lambda x : Counter(kkma.morphs(x)))\n",
    "    voc_tok = voc_testset['VOC'].progress_apply(lambda x : Counter(kkma.morphs(x)))\n",
    "    filt4 = voc_tok.isin(voc_etc).astype(int)\n",
    "    voc_testset = voc_testset[~filt4.astype(bool)].reset_index()\n",
    "    \n",
    "    #Setup\n",
    "    print('Setting : ' + file)\n",
    "    data_module = VOC_DataModule(voc_testset, voc_testset, tokenizer, batch_size=BATCH_SIZE, max_token_len=MAX_LEN)\n",
    "    data_module.setup()\n",
    "    trainer = L.Trainer(max_epochs=N_EPOCHS)\n",
    "    \n",
    "    #Inference\n",
    "    print('Inferencing : ' + file)\n",
    "    testing_predict = trainer.predict(new_model, datamodule=data_module)\n",
    "    sema_df_final = np.vstack(testing_predict)\n",
    "    pred_label = (sema_df_final>opt_thresh).astype(int)\n",
    "    voc_testset['pred'] = pd.Series(mlb.inverse_transform(pred_label)).apply(list)\n",
    "    voc_testset = voc_testset.explode('pred',ignore_index=True)\n",
    "    \n",
    "    del voc_testset['label']\n",
    "    \n",
    "    #키워드\n",
    "    print('Extracting Keywords : ' + file)\n",
    "    voc_testset['topic'] = voc_testset.pred.str.split('_').str[0]\n",
    "    voc_testset['sentiment'] = voc_testset.pred.str.split('_').str[1]\n",
    "    voc_testset.topic.fillna('기타',inplace=True)\n",
    "    voc_testset['keyword'] = keyword.loc[voc_testset.topic].values\n",
    "    voc_testset['keyword'] = voc_testset.apply(findall_vec2, axis=1)\n",
    "    \n",
    "    # #save\n",
    "    print('Saving output File : ' + file)\n",
    "    voc_testset.to_excel('data/output/' + file[:-5] +'_output.xlsx')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609dede9-cbb6-4ee7-8044-2ffdf9ea689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_testset.iloc[:,2:].to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e790e6-c855-43ea-959f-4a0dcec9edcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: docker: command not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway --name open-webui ghcr.io/open-webui/open-webui:main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec38e47-6974-418c-8382-d43f584107a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3a6f087-81ca-46e5-927e-8a28215fe266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: docker: command not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1fb82-759f-4ba3-9126-65d5bf750ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
