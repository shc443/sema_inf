version: '3.8'

services:
  sema:
    build:
      context: .
      dockerfile: Dockerfile
    image: sema-inference:latest
    container_name: sema-inference

    # GPU support (requires nvidia-docker)
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

    # Volume mounts for data I/O
    volumes:
      - ./data/input:/workspace/data/input
      - ./data/output:/workspace/data/output
      - ./model:/workspace/model
      - ./cache:/workspace/cache

    # Keep container running for manual execution
    stdin_open: true
    tty: true

    # Resource limits (adjust based on your system)
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
