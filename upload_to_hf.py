#!/usr/bin/env python3
"""
Script to help you upload model files to Hugging Face Hub
Run this locally to upload your model files to your HF repository
"""

import os
from huggingface_hub import HfApi, create_repo, upload_file
from pathlib import Path

def upload_to_huggingface(repo_id="shc443/sema-model", token=None):
    """
    Upload model files to Hugging Face Hub
    
    Args:
        repo_id: Your Hugging Face repository ID (username/repo-name)
        token: Your Hugging Face token (will prompt if not provided)
    """
    
    # Initialize HF API
    api = HfApi()
    
    # Login if token provided, otherwise will use saved token or prompt
    if token:
        api.token = token
    
    print(f"üöÄ Uploading model files to {repo_id}")
    
    # Create repository if it doesn't exist
    try:
        create_repo(repo_id, exist_ok=True, repo_type="model")
        print(f"‚úÖ Repository {repo_id} ready")
    except Exception as e:
        print(f"Repository creation failed: {e}")
        return False
    
    # Files to upload
    files_to_upload = [
        "data/data2.pkl",
        "data/voc_etc.pkl", 
        "data/keyword_doc.pkl",
        # Add your checkpoint file here
        "model/deberta-v3-xlarge-korean_20ep_full_mar17_dropna.ckpt"
    ]
    
    # Find checkpoint file with different possible extensions
    checkpoint_patterns = [
        "model/*.ckpt",
        "model/*_20ep_full_mar17_dropna.ckpt",
        "model/deberta*.ckpt"
    ]
    
    import glob
    checkpoint_found = False
    for pattern in checkpoint_patterns:
        matches = glob.glob(pattern)
        if matches:
            # Remove duplicates and add any additional checkpoints found
            for match in matches:
                if match not in files_to_upload:
                    files_to_upload.append(match)
            checkpoint_found = True
            print(f"Found checkpoint(s): {matches}")
    
    if not checkpoint_found:
        print("‚ö†Ô∏è Warning: No checkpoint file found in model/ directory. Please check the path.")
    
    # Upload each file
    uploaded_files = []
    failed_files = []
    
    for local_path in files_to_upload:
        if not os.path.exists(local_path):
            print(f"‚ö†Ô∏è File not found: {local_path}")
            failed_files.append(local_path)
            continue
            
        try:
            # Get file size for progress
            file_size = os.path.getsize(local_path)
            print(f"üì§ Uploading {local_path} ({file_size / 1e6:.1f} MB)...")
            
            # Upload file
            upload_file(
                path_or_fileobj=local_path,
                path_in_repo=os.path.basename(local_path),  # Upload with just the filename
                repo_id=repo_id,
                repo_type="model"
            )
            
            uploaded_files.append(local_path)
            print(f"‚úÖ Uploaded {local_path}")
            
        except Exception as e:
            print(f"‚ùå Failed to upload {local_path}: {e}")
            failed_files.append(local_path)
    
    # Create README for the repository
    readme_content = f"""---
title: SEMA Korean VOC Analysis Model
tags:
- korean
- sentiment-analysis
- voice-of-customer
- deberta
- pytorch
license: mit
---

# SEMA Korean VOC Analysis Model

This repository contains the trained model and data files for Korean Voice of Customer (VOC) sentiment analysis using DeBERTa.

## Files

- `data2.pkl`: MultiLabelBinarizer for label encoding
- `voc_etc.pkl`: Filter data for VOC preprocessing  
- `keyword_doc.pkl`: Keyword extraction mappings
- `*.ckpt`: Trained model checkpoint

## Usage

```python
from src.cli import SemaInference

# Initialize with auto-download from this repository
inferencer = SemaInference(hf_repo="{repo_id}")

# Process Excel file
inferencer.process_file("input.xlsx", "output.xlsx")
```

## Model Details

- Base Model: team-lucid/deberta-v3-xlarge-korean
- Task: Multi-label classification for Korean VOC analysis
- Framework: PyTorch Lightning

## Requirements

- torch
- transformers
- huggingface_hub
- konlpy (requires Java)
- pandas, numpy, scikit-learn

Auto-generated by SEMA inference CLI.
"""
    
    try:
        # Upload README
        with open("temp_README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)
        
        upload_file(
            path_or_fileobj="temp_README.md",
            path_in_repo="README.md",
            repo_id=repo_id,
            repo_type="model"
        )
        
        os.remove("temp_README.md")
        print("‚úÖ README.md uploaded")
        
    except Exception as e:
        print(f"‚ö†Ô∏è README upload failed: {e}")
    
    # Summary
    print(f"\nüìä Upload Summary:")
    print(f"‚úÖ Successfully uploaded: {len(uploaded_files)} files")
    if uploaded_files:
        for f in uploaded_files:
            print(f"   - {f}")
    
    if failed_files:
        print(f"‚ùå Failed uploads: {len(failed_files)} files")
        for f in failed_files:
            print(f"   - {f}")
    
    print(f"\nüîó Repository: https://huggingface.co/{repo_id}")
    
    return len(failed_files) == 0

def main():
    """Main function with instructions"""
    print("""
ü§ó Hugging Face Model Upload Script

This script will upload your SEMA model files to Hugging Face Hub.

Prerequisites:
1. Install huggingface_hub: pip install huggingface_hub
2. Login to HF: huggingface-cli login
3. Have your model files ready:
   - data/data2.pkl
   - data/voc_etc.pkl  
   - data/keyword_doc.pkl
   - your_model_checkpoint.ckpt

Usage:
   python upload_to_hf.py
   
Or customize:
   upload_to_huggingface("your-username/your-repo-name")
""")
    
    # Check if user is logged in
    try:
        api = HfApi()
        user = api.whoami()
        print(f"‚úÖ Logged in as: {user['name']}")
    except Exception:
        print("‚ùå Not logged in to Hugging Face.")
        print("Please run: huggingface-cli login")
        return
    
    # Run upload
    success = upload_to_huggingface()
    
    if success:
        print("\nüéâ All files uploaded successfully!")
        print("Your Google Colab setup will now automatically download these files.")
    else:
        print("\n‚ö†Ô∏è Some files failed to upload. Please check the errors above.")

if __name__ == "__main__":
    main()