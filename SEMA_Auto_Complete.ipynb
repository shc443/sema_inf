{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ SEMA Auto-Complete VOC Analysis\n",
    "\n",
    "**Fully Automated Korean Voice of Customer Classification**\n",
    "\n",
    "## Instructions:\n",
    "1. Set runtime to **GPU** (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "2. Upload Excel files to `/content/data/input/` folder BEFORE running\n",
    "3. **Run ALL cells** - completely automated processing\n",
    "4. Results automatically download when complete\n",
    "\n",
    "## Features:\n",
    "- üß† **Auto GPU Detection**: Selects optimal model based on available VRAM\n",
    "- üìÅ **Auto File Processing**: Processes all files in input folder\n",
    "- üõ°Ô∏è **Safety Features**: Timeout protection and error handling\n",
    "- üì• **Auto Download**: Results download automatically\n",
    "\n",
    "## File Requirements:\n",
    "- Excel files with **VOC1** and **VOC2** columns\n",
    "- Korean text content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup & Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Setting up SEMA Auto-Complete environment...\")\n",
    "\n",
    "# Install system dependencies\n",
    "!apt-get update -qq && apt-get install -y openjdk-8-jdk -qq\n",
    "\n",
    "# Set Java environment\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "\n",
    "# Install packages\n",
    "!pip install -q \"huggingface_hub>=0.16.0\" \"torch>=2.0.0\" \"transformers>=4.30.0,<5.0.0\" \"torchmetrics>=0.11.0\" \"lightning>=2.0.0\" konlpy psutil\n",
    "\n",
    "# Setup repository\n",
    "!git clone -q https://github.com/shc443/sema_inf.git\n",
    "%cd sema_inf\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"‚úÖ Basic setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† GPU Detection & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_gpu_and_select_model():\n",
    "    \"\"\"Detect GPU memory and select appropriate model size\"\"\"\n",
    "    \n",
    "    # Check GPU availability\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"‚ùå No GPU detected! Please change runtime to GPU.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Get GPU info\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    \n",
    "    print(f\"üöÄ GPU Detected: {gpu_name}\")\n",
    "    print(f\"üìä GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "    \n",
    "    # System info\n",
    "    cpu_count = psutil.cpu_count()\n",
    "    ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    print(f\"üñ•Ô∏è System: {cpu_count} cores, {ram_gb:.1f}GB RAM\")\n",
    "    \n",
    "    # Model selection based on GPU memory\n",
    "    if gpu_memory_gb >= 15.0:  # High-end GPU (T4, V100, A100)\n",
    "        model_size = \"xlarge\"\n",
    "        checkpoint_file = \"deberta-v3-xlarge-korean_20ep_full_mar17_dropna.ckpt\"\n",
    "        batch_size = 8\n",
    "        print(f\"üéØ Selected: XLarge model (optimal for {gpu_memory_gb:.1f}GB)\")\n",
    "        \n",
    "    elif gpu_memory_gb >= 8.0:  # Medium GPU (might work with xlarge)\n",
    "        model_size = \"xlarge\"\n",
    "        checkpoint_file = \"deberta-v3-xlarge-korean_20ep_full_mar17_dropna.ckpt\"\n",
    "        batch_size = 4  # Smaller batch for safety\n",
    "        print(f\"‚ö†Ô∏è Selected: XLarge model with reduced batch size ({gpu_memory_gb:.1f}GB)\")\n",
    "        print(\"üí° Will fallback to small model if memory issues occur\")\n",
    "        \n",
    "    else:  # Low memory GPU\n",
    "        model_size = \"small\"\n",
    "        checkpoint_file = \"deberta-v3-small-korean_20ep_full_mar17_dropna.ckpt\"  # Future small model\n",
    "        batch_size = 8\n",
    "        print(f\"üîß Selected: Small model (optimized for {gpu_memory_gb:.1f}GB)\")\n",
    "    \n",
    "    return {\n",
    "        'model_size': model_size,\n",
    "        'checkpoint_file': checkpoint_file,\n",
    "        'batch_size': batch_size,\n",
    "        'gpu_memory': gpu_memory_gb,\n",
    "        'gpu_name': gpu_name\n",
    "    }, None\n",
    "\n",
    "# Detect and select model\n",
    "gpu_config, error = detect_gpu_and_select_model()\n",
    "if error:\n",
    "    print(f\"‚ùå Error: {error}\")\n",
    "    raise Exception(error)\n",
    "\n",
    "print(f\"\\n‚úÖ Configuration ready!\")\n",
    "print(f\"   Model: {gpu_config['model_size']}\")\n",
    "print(f\"   Batch Size: {gpu_config['batch_size']}\")\n",
    "print(f\"   Checkpoint: {gpu_config['checkpoint_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Initialize Directories & Check Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs('data/input', exist_ok=True)\n",
    "os.makedirs('data/output', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('logs/errors', exist_ok=True)\n",
    "\n",
    "# Check for input files\n",
    "input_dir = Path('data/input')\n",
    "excel_files = list(input_dir.glob('*.xlsx'))\n",
    "\n",
    "print(f\"üìÅ Directories created\")\n",
    "print(f\"üìä Input files found: {len(excel_files)}\")\n",
    "\n",
    "if len(excel_files) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è No Excel files found in data/input/ directory!\")\n",
    "    print(\"üì§ Please upload your Excel files to /content/data/input/ before continuing\")\n",
    "    print(\"üí° Use the file browser on the left or the upload button\")\n",
    "    \n",
    "    # Show how to upload files\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(\"\"\"\n",
    "    <div style=\"background: #fff3cd; padding: 15px; border: 1px solid #ffeaa7; border-radius: 5px; margin: 10px 0;\">\n",
    "        <h4>üì§ How to Upload Files:</h4>\n",
    "        <ol>\n",
    "            <li>Click the <strong>üìÅ folder icon</strong> in the left sidebar</li>\n",
    "            <li>Navigate to <code>data/input/</code> folder</li>\n",
    "            <li>Click <strong>Upload</strong> button and select your Excel files</li>\n",
    "            <li>Wait for upload to complete</li>\n",
    "            <li>Re-run this cell to verify files are uploaded</li>\n",
    "        </ol>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Don't proceed without files\n",
    "    raise Exception(\"Please upload Excel files to data/input/ directory first\")\n",
    "else:\n",
    "    print(\"\\nüìã Files to process:\")\n",
    "    for i, file in enumerate(excel_files, 1):\n",
    "        print(f\"   {i}. {file.name}\")\n",
    "    print(\"\\n‚úÖ Ready to process!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Load SEMA Model & Start Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "\n",
    "class AutoSemaProcessor:\n",
    "    def __init__(self, gpu_config):\n",
    "        self.gpu_config = gpu_config\n",
    "        self.start_time = time.time()\n",
    "        self.processed_files = []\n",
    "        self.failed_files = []\n",
    "        \n",
    "    def load_sema_with_fallback(self):\n",
    "        \"\"\"Load SEMA with automatic fallback to smaller model if needed\"\"\"\n",
    "        print(f\"üîÑ Loading SEMA with {self.gpu_config['model_size']} model...\")\n",
    "        \n",
    "        try:\n",
    "            from colab_cli import SemaColabCLI\n",
    "            \n",
    "            # Try to initialize with selected model\n",
    "            sema = SemaColabCLI()\n",
    "            sema.batch_size = self.gpu_config['batch_size']\n",
    "            \n",
    "            # Test with small input to check memory\n",
    "            if self.gpu_config['model_size'] == 'xlarge' and self.gpu_config['gpu_memory'] < 12.0:\n",
    "                print(\"üß™ Testing XLarge model with limited memory...\")\n",
    "                torch.cuda.empty_cache()  # Clear cache first\n",
    "            \n",
    "            print(f\"‚úÖ SEMA loaded successfully!\")\n",
    "            print(f\"   Model: {self.gpu_config['model_size']}\")\n",
    "            print(f\"   Batch Size: {sema.batch_size}\")\n",
    "            return sema\n",
    "            \n",
    "        except Exception as e:\n",
    "            if 'out of memory' in str(e).lower() or 'cuda' in str(e).lower():\n",
    "                print(f\"‚ùå Memory error with {self.gpu_config['model_size']} model: {e}\")\n",
    "                \n",
    "                if self.gpu_config['model_size'] == 'xlarge':\n",
    "                    print(\"üîÑ Falling back to small model...\")\n",
    "                    \n",
    "                    # Update config for small model\n",
    "                    self.gpu_config['model_size'] = 'small'\n",
    "                    self.gpu_config['checkpoint_file'] = 'deberta-v3-small-korean_20ep_full_mar17_dropna.ckpt'\n",
    "                    self.gpu_config['batch_size'] = 8\n",
    "                    \n",
    "                    # Clear GPU memory\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # Try again with small model\n",
    "                    try:\n",
    "                        sema = SemaColabCLI()\n",
    "                        sema.batch_size = self.gpu_config['batch_size']\n",
    "                        print(\"‚úÖ Small model loaded successfully!\")\n",
    "                        return sema\n",
    "                    except Exception as e2:\n",
    "                        print(f\"‚ùå Small model also failed: {e2}\")\n",
    "                        raise e2\n",
    "                else:\n",
    "                    print(\"‚ùå Small model failed - no fallback available\")\n",
    "                    raise e\n",
    "            else:\n",
    "                print(f\"‚ùå Unexpected error: {e}\")\n",
    "                raise e\n",
    "    \n",
    "    def process_all_files(self):\n",
    "        \"\"\"Process all files in input directory\"\"\"\n",
    "        try:\n",
    "            # Load SEMA\n",
    "            sema = self.load_sema_with_fallback()\n",
    "            \n",
    "            # Get input files\n",
    "            input_files = list(Path('data/input').glob('*.xlsx'))\n",
    "            total_files = len(input_files)\n",
    "            \n",
    "            print(f\"\\nüöÄ Starting processing of {total_files} files...\")\n",
    "            print(f\"‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            \n",
    "            for i, input_file in enumerate(input_files, 1):\n",
    "                print(f\"\\nüìÑ Processing file {i}/{total_files}: {input_file.name}\")\n",
    "                file_start = time.time()\n",
    "                \n",
    "                try:\n",
    "                    # Process single file\n",
    "                    output_file = f\"data/output/{input_file.stem}_output.xlsx\"\n",
    "                    success = sema.process_file(str(input_file), output_file)\n",
    "                    \n",
    "                    if success:\n",
    "                        elapsed = time.time() - file_start\n",
    "                        self.processed_files.append(input_file.name)\n",
    "                        print(f\"‚úÖ {input_file.name} completed in {elapsed:.1f}s\")\n",
    "                    else:\n",
    "                        self.failed_files.append(input_file.name)\n",
    "                        print(f\"‚ùå {input_file.name} failed\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    self.failed_files.append(input_file.name)\n",
    "                    print(f\"‚ùå Error processing {input_file.name}: {e}\")\n",
    "                    \n",
    "                    # Log error\n",
    "                    error_log = {\n",
    "                        'file': input_file.name,\n",
    "                        'error': str(e),\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'traceback': traceback.format_exc()\n",
    "                    }\n",
    "                    \n",
    "                    with open(f'logs/errors/{input_file.stem}_error.json', 'w') as f:\n",
    "                        import json\n",
    "                        json.dump(error_log, f, indent=2)\n",
    "            \n",
    "            # Summary\n",
    "            total_time = time.time() - self.start_time\n",
    "            print(f\"\\nüéâ Processing Summary:\")\n",
    "            print(f\"   ‚úÖ Successful: {len(self.processed_files)} files\")\n",
    "            print(f\"   ‚ùå Failed: {len(self.failed_files)} files\")\n",
    "            print(f\"   ‚è∞ Total time: {total_time:.1f}s\")\n",
    "            print(f\"   üß† Model used: {self.gpu_config['model_size']}\")\n",
    "            \n",
    "            if self.failed_files:\n",
    "                print(f\"\\n‚ùå Failed files: {', '.join(self.failed_files)}\")\n",
    "                print(f\"üìÅ Error logs saved in logs/errors/ directory\")\n",
    "            \n",
    "            return len(self.processed_files) > 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Critical error in processing: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "# Initialize and run processor\n",
    "processor = AutoSemaProcessor(gpu_config)\n",
    "success = processor.process_all_files()\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéä Processing completed successfully!\")\n",
    "else:\n",
    "    print(\"\\nüí• Processing failed or no files processed\")\n",
    "    print(\"üîç Check error logs for details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Auto-Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output files\n",
    "output_files = list(Path('data/output').glob('*.xlsx'))\n",
    "\n",
    "if len(output_files) > 0:\n",
    "    print(f\"üì• Downloading {len(output_files)} result files...\")\n",
    "    \n",
    "    # Download all output files\n",
    "    for output_file in output_files:\n",
    "        try:\n",
    "            files.download(str(output_file))\n",
    "            print(f\"‚úÖ Downloaded: {output_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to download {output_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ All {len(output_files)} files downloaded to your computer!\")\n",
    "    print(\"üìÅ Check your Downloads folder\")\n",
    "    \n",
    "    # Show file details\n",
    "    print(\"\\nüìã Downloaded files:\")\n",
    "    for i, file in enumerate(output_files, 1):\n",
    "        print(f\"   {i}. {file.name}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No output files found to download\")\n",
    "    print(\"üîç Check processing logs above for errors\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nüèÅ SEMA Auto-Complete finished!\")\n",
    "print(f\"   üìä Processed: {len(processor.processed_files)} files\")\n",
    "print(f\"   üì• Downloaded: {len(output_files)} files\")\n",
    "print(f\"   üß† Model used: {gpu_config['model_size']}\")\n",
    "\n",
    "if len(processor.failed_files) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è Failed: {len(processor.failed_files)} files (check error logs)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}