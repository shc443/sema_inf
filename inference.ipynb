{
 "cells": [
  {
   "cell_type": "code",
   "id": "ce353533-804f-4618-ae21-8bfb7c67ac88",
   "metadata": {},
   "outputs": [],
   "source": "# SEMA Model Inference Setup\nfrom sema import VOC_TopicLabeler, VOC_DataModule\nfrom transformers import AutoTokenizer, AutoConfig\nimport lightning as L\nimport os, pickle, re\nimport numpy as np\nimport pandas as pd \nfrom konlpy.tag import Kkma\nfrom tqdm import tqdm\nfrom collections import Counter\n\nkkma = Kkma()\n\ndef findall_vec(key, voc):\n    try:\n        return re.findall(key, voc)[0]\n    except:\n        return ''\n\ndef findall_vec2(df):\n    return findall_vec(df['keyword'], df['VOC'])\n\ndef remove_non_english_korean(string):\n    pattern = re.compile(r'[^a-zA-Z0-9\\uac00-\\ud7a3\\s]', flags=re.UNICODE)\n    return pattern.sub('', string)\n    \ndef strip_e(st):\n    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n    return RE_EMOJI.sub(r'', st)"
  },
  {
   "cell_type": "code",
   "id": "47e29e06-9ec3-442f-bd7c-fc202782319c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "# Model Configuration and Loading\nBATCH_SIZE = 12\nMAX_LEN = 256\nopt_thresh = 0.5\n\n# Load files and model components\ninput_file = pd.Series(os.listdir('./data/input'))\ninput_file = input_file[input_file.str.contains('.xlsx')]\n\noutput_list = pd.Series(os.listdir('./data/output'))\nrunning_file = input_file[~input_file.str[:-5].isin(output_list.str.split('_output').str[0])]\n\nconfig = AutoConfig.from_pretrained('team-lucid/deberta-v3-xlarge-korean', output_hidden_states=True)\ntokenizer = AutoTokenizer.from_pretrained('team-lucid/deberta-v3-xlarge-korean')\n\nwith open('data/data2.pkl', 'rb') as f:\n    mlb = pickle.load(f)\n\nLABEL_COLUMNS = mlb.classes_\nvoc_etc = pd.read_pickle('data/voc_etc.pkl')\nkeyword = pd.read_pickle('data/keyword_doc.pkl')\n\nnew_model = VOC_TopicLabeler.load_from_checkpoint(\n    checkpoint_path=\"model/deberta-v3-xlarge-korean_20ep_full_mar17_dropna.ckpt\", \n    n_classes=18, \n    model='team-lucid/deberta-v3-xlarge-korean'\n).cuda()\nnew_model.eval()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9084143-e92c-4c6e-b2ad-9d2490fd393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.classes_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d06b1f4-4fd3-4d78-80cf-b2dbb8a7f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_file"
   ]
  },
  {
   "cell_type": "code",
   "id": "06781d6c-bb49-45c4-8510-d9940c1f23e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "# Process Files\nfor file in running_file:\n    print(f'Processing: {file}')\n    \n    # Read and prepare data\n    voc_testset = pd.read_excel(f\"data/input/{file}\", dtype=str)\n    voc = pd.concat([voc_testset.VOC1, voc_testset.VOC2]).sort_index().values\n    voc_testset = pd.concat([voc_testset]*2).sort_index().iloc[:,1:-2]\n    voc_testset['VOC'] = voc\n    voc_testset = voc_testset.dropna(subset='VOC').reset_index()\n    voc_testset['label'] = pd.DataFrame(np.zeros((18, voc_testset.shape[0])).T).astype(int).apply(list, axis=1)\n    \n    # Clean and filter data\n    voc_testset = voc_testset[voc_testset['VOC'] != 'nan']\n    voc_testset['VOC'] = voc_testset['VOC'].apply(remove_non_english_korean).apply(strip_e)\n    voc_testset['VOC'] = voc_testset['VOC'].replace(r'\\s+', ' ', regex=True)\n    \n    # Apply filters\n    filt0 = (voc_testset['VOC'].str.strip().str.len() < 4).astype(int)\n    filt1 = voc_testset['VOC'].apply(lambda x: bool(re.match(r'^[_\\W]+$', str(x).replace(' ','')))).astype(int)\n    filt2 = voc_testset['VOC'].apply(lambda x: bool(re.match(r'[\\d/-]+$', str(x).replace(' ','')))).astype(int)\n    filt3 = (voc_testset.VOC.str.replace(' ','').str.split('').apply(set).str.len() == 2)\n    voc_testset = voc_testset[(filt0 + filt1 + filt2 + filt3) == 0]\n    \n    voc_tok = voc_testset['VOC'].progress_apply(lambda x: Counter(kkma.morphs(x)))\n    filt4 = voc_tok.isin(voc_etc).astype(int)\n    voc_testset = voc_testset[~filt4.astype(bool)].reset_index()\n    \n    # Run inference\n    data_module = VOC_DataModule(voc_testset, voc_testset, tokenizer, batch_size=BATCH_SIZE, max_token_len=MAX_LEN)\n    data_module.setup()\n    trainer = L.Trainer(max_epochs=35)\n    \n    testing_predict = trainer.predict(new_model, datamodule=data_module)\n    sema_df_final = np.vstack(testing_predict)\n    pred_label = (sema_df_final > opt_thresh).astype(int)\n    voc_testset['pred'] = pd.Series(mlb.inverse_transform(pred_label)).apply(list)\n    voc_testset = voc_testset.explode('pred', ignore_index=True)\n    \n    del voc_testset['label']\n    \n    # Extract keywords and topics\n    voc_testset['topic'] = voc_testset.pred.str.split('_').str[0]\n    voc_testset['sentiment'] = voc_testset.pred.str.split('_').str[1]\n    voc_testset.topic = voc_testset.topic.fillna('기타')\n    voc_testset['keyword'] = keyword.loc[voc_testset.topic].values\n    voc_testset['keyword'] = voc_testset.apply(findall_vec2, axis=1)\n    \n    # Save output\n    voc_testset.to_excel(f'data/output/{file[:-5]}_output.xlsx')\n    print(f'Completed: {file}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}