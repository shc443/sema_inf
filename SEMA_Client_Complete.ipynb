{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ SEMA Client - Complete VOC Analysis\n",
    "\n",
    "**Zero-Setup Korean Voice of Customer Classification for Clients**\n",
    "\n",
    "## üéØ For Clients - No Setup Required!\n",
    "\n",
    "1. **Set Runtime to GPU** (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "2. **Upload Excel files** to `/content/sema/data/input/` folder\n",
    "3. **Run all cells** - everything is automatic!\n",
    "4. **Download results** - automatic at the end\n",
    "\n",
    "## ‚ú® What This Does:\n",
    "- ü§ñ **Auto-detects GPU** and selects best model (small/xlarge)\n",
    "- üì• **Auto-downloads** model files from Hugging Face\n",
    "- üíæ **Caches in Google Drive** for faster future runs\n",
    "- üìä **Processes all Excel files** automatically\n",
    "- üìÅ **Organizes input/output** in clean folders\n",
    "- üì• **Auto-downloads results** to your computer\n",
    "\n",
    "## üìã File Requirements:\n",
    "- Excel files (.xlsx) with **VOC1** and **VOC2** columns\n",
    "- Korean text content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Environment Setup & GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ SEMA Client - Initializing complete environment...\")\n",
    "\n",
    "# Install system dependencies\n",
    "!apt-get update -qq && apt-get install -y openjdk-8-jdk -qq\n",
    "\n",
    "# Set Java environment for Korean NLP\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "\n",
    "# Install Python packages\n",
    "!pip install -q \"huggingface_hub>=0.16.0\" \"torch>=2.0.0\" \"transformers>=4.30.0,<5.0.0\" \"torchmetrics>=0.11.0\" \"lightning>=2.0.0\" konlpy psutil\n",
    "\n",
    "# Clone SEMA repository\n",
    "!git clone -q https://github.com/shc443/sema_inf.git\n",
    "%cd sema_inf\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"‚úÖ Basic setup complete!\")\n",
    "\n",
    "# GPU Detection and Model Selection\n",
    "import torch\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_optimal_model():\n",
    "    \"\"\"Detect GPU and select optimal model configuration\"\"\"\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"‚ùå No GPU detected! Please set Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "        return None\n",
    "    \n",
    "    # Get GPU information\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    \n",
    "    # System information\n",
    "    cpu_count = psutil.cpu_count()\n",
    "    ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    \n",
    "    print(f\"üñ•Ô∏è System Detected:\")\n",
    "    print(f\"   GPU: {gpu_name}\")\n",
    "    print(f\"   VRAM: {gpu_memory_gb:.1f} GB\")\n",
    "    print(f\"   CPU: {cpu_count} cores\")\n",
    "    print(f\"   RAM: {ram_gb:.1f} GB\")\n",
    "    \n",
    "    # Model selection logic\n",
    "    if gpu_memory_gb >= 14.0:  # High-end GPU (T4, V100, A100, etc.)\n",
    "        model_config = {\n",
    "            'size': 'xlarge',\n",
    "            'checkpoint': 'deberta-v3-xlarge-korean_20ep_full_mar17_dropna.ckpt',\n",
    "            'batch_size': 8,\n",
    "            'reason': 'High VRAM - optimal performance'\n",
    "        }\n",
    "    elif gpu_memory_gb >= 10.0:  # Medium GPU\n",
    "        model_config = {\n",
    "            'size': 'xlarge',\n",
    "            'checkpoint': 'deberta-v3-xlarge-korean_20ep_full_mar17_dropna.ckpt',\n",
    "            'batch_size': 4,\n",
    "            'reason': 'Medium VRAM - reduced batch size'\n",
    "        }\n",
    "    elif gpu_memory_gb >= 8.0:  # Standard Colab free tier\n",
    "        model_config = {\n",
    "            'size': 'small',\n",
    "            'checkpoint': 'deberta-v3-small-korean_20ep_full_mar17_dropna.ckpt',\n",
    "            'batch_size': 8,\n",
    "            'reason': 'Standard VRAM - optimized small model'\n",
    "        }\n",
    "    else:  # Low memory\n",
    "        model_config = {\n",
    "            'size': 'small',\n",
    "            'checkpoint': 'deberta-v3-small-korean_20ep_full_mar17_dropna.ckpt',\n",
    "            'batch_size': 4,\n",
    "            'reason': 'Limited VRAM - conservative settings'\n",
    "        }\n",
    "    \n",
    "    model_config['gpu_memory'] = gpu_memory_gb\n",
    "    model_config['gpu_name'] = gpu_name\n",
    "    \n",
    "    print(f\"\\nüéØ Selected Configuration:\")\n",
    "    print(f\"   Model: {model_config['size']} ({model_config['reason']})\")\n",
    "    print(f\"   Batch Size: {model_config['batch_size']}\")\n",
    "    print(f\"   Checkpoint: {model_config['checkpoint']}\")\n",
    "    \n",
    "    return model_config\n",
    "\n",
    "# Detect optimal configuration\n",
    "MODEL_CONFIG = detect_optimal_model()\n",
    "if not MODEL_CONFIG:\n",
    "    raise Exception(\"GPU detection failed. Please enable GPU runtime.\")\n",
    "\n",
    "print(\"\\n‚úÖ GPU detection and model selection complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 2: Google Drive Setup & Model Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from huggingface_hub import hf_hub_download\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Mount Google Drive for caching\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    DRIVE_CACHE = Path('/content/drive/MyDrive/SEMA_Cache')\n",
    "    DRIVE_CACHE.mkdir(exist_ok=True)\n",
    "    print(\"‚úÖ Google Drive mounted for model caching\")\n",
    "    USE_DRIVE_CACHE = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Google Drive mount failed: {e}\")\n",
    "    print(\"üì• Will download models directly (slower on repeated runs)\")\n",
    "    DRIVE_CACHE = None\n",
    "    USE_DRIVE_CACHE = False\n",
    "\n",
    "def download_model_files(config, hf_repo=\"shc443/sema2025\"):\n",
    "    \"\"\"Download model files with Google Drive caching\"\"\"\n",
    "    \n",
    "    print(f\"üì• Setting up model files for {config['size']} model...\")\n",
    "    \n",
    "    # Create local directories\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    os.makedirs('model', exist_ok=True)\n",
    "    \n",
    "    # Files to download\n",
    "    required_files = [\n",
    "        ('data2.pkl', 'data/data2.pkl'),\n",
    "        ('voc_etc.pkl', 'data/voc_etc.pkl'),\n",
    "        ('keyword_doc.pkl', 'data/keyword_doc.pkl'),\n",
    "        (config['checkpoint'], f\"model/{config['checkpoint']}\")\n",
    "    ]\n",
    "    \n",
    "    downloaded_count = 0\n",
    "    \n",
    "    for hf_filename, local_path in required_files:\n",
    "        local_file = Path(local_path)\n",
    "        cache_file = DRIVE_CACHE / hf_filename if USE_DRIVE_CACHE else None\n",
    "        \n",
    "        try:\n",
    "            # Check if file exists in Drive cache\n",
    "            if USE_DRIVE_CACHE and cache_file and cache_file.exists():\n",
    "                print(f\"üìÇ Found in Drive cache: {hf_filename}\")\n",
    "                shutil.copy2(cache_file, local_file)\n",
    "                print(f\"‚úÖ Copied from cache: {hf_filename}\")\n",
    "                downloaded_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Download from Hugging Face\n",
    "            print(f\"üì• Downloading from HF: {hf_filename}...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            downloaded_path = hf_hub_download(\n",
    "                repo_id=hf_repo,\n",
    "                filename=hf_filename,\n",
    "                cache_dir=\"./hf_cache\",\n",
    "                resume=True\n",
    "            )\n",
    "            \n",
    "            # Copy to local path\n",
    "            local_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(downloaded_path, local_file)\n",
    "            \n",
    "            # Cache in Google Drive if available\n",
    "            if USE_DRIVE_CACHE and cache_file:\n",
    "                try:\n",
    "                    shutil.copy2(downloaded_path, cache_file)\n",
    "                    print(f\"üíæ Cached to Drive: {hf_filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Drive cache failed for {hf_filename}: {e}\")\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            file_size = local_file.stat().st_size / (1024*1024)  # MB\n",
    "            print(f\"‚úÖ Downloaded: {hf_filename} ({file_size:.1f}MB in {elapsed:.1f}s)\")\n",
    "            downloaded_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to download {hf_filename}: {e}\")\n",
    "            \n",
    "            # Try fallback to xlarge if small model fails\n",
    "            if 'small' in hf_filename and config['size'] == 'small':\n",
    "                print(f\"üîÑ Small model not found, trying xlarge model...\")\n",
    "                fallback_filename = hf_filename.replace('small', 'xlarge')\n",
    "                try:\n",
    "                    downloaded_path = hf_hub_download(\n",
    "                        repo_id=hf_repo,\n",
    "                        filename=fallback_filename,\n",
    "                        cache_dir=\"./hf_cache\",\n",
    "                        resume=True\n",
    "                    )\n",
    "                    shutil.copy2(downloaded_path, local_file)\n",
    "                    print(f\"‚úÖ Using xlarge model as fallback\")\n",
    "                    # Update config\n",
    "                    config['size'] = 'xlarge'\n",
    "                    config['batch_size'] = min(config['batch_size'], 4)  # Reduce batch size\n",
    "                    downloaded_count += 1\n",
    "                except Exception as e2:\n",
    "                    print(f\"‚ùå Fallback also failed: {e2}\")\n",
    "                    return False\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "    print(f\"\\nüìä Download Summary:\")\n",
    "    print(f\"   ‚úÖ Successfully downloaded: {downloaded_count}/{len(required_files)} files\")\n",
    "    print(f\"   üíæ Drive cache: {'Enabled' if USE_DRIVE_CACHE else 'Disabled'}\")\n",
    "    print(f\"   üéØ Model ready: {config['size']}\")\n",
    "    \n",
    "    return downloaded_count == len(required_files)\n",
    "\n",
    "# Download all required files\n",
    "download_success = download_model_files(MODEL_CONFIG)\n",
    "\n",
    "if not download_success:\n",
    "    raise Exception(\"Failed to download required model files\")\n",
    "\n",
    "print(\"\\nüéâ All model files ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Step 3: Setup Workspace & Check Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create organized workspace\n",
    "WORKSPACE = Path('/content/sema')\n",
    "INPUT_DIR = WORKSPACE / 'data' / 'input'\n",
    "OUTPUT_DIR = WORKSPACE / 'data' / 'output'\n",
    "LOGS_DIR = WORKSPACE / 'logs'\n",
    "\n",
    "# Create directories\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(LOGS_DIR / 'errors').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Workspace created at: {WORKSPACE}\")\n",
    "print(f\"   üìÇ Input folder: {INPUT_DIR}\")\n",
    "print(f\"   üìÇ Output folder: {OUTPUT_DIR}\")\n",
    "print(f\"   üìÇ Logs folder: {LOGS_DIR}\")\n",
    "\n",
    "# Check for input files\n",
    "excel_files = list(INPUT_DIR.glob('*.xlsx'))\n",
    "print(f\"\\nüìä Input files found: {len(excel_files)}\")\n",
    "\n",
    "if len(excel_files) == 0:\n",
    "    from IPython.display import display, HTML\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è No Excel files found!\")\n",
    "    \n",
    "    # Show upload instructions\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"background: #e3f2fd; padding: 20px; border: 2px solid #2196f3; border-radius: 10px; margin: 15px 0;\">\n",
    "        <h3 style=\"color: #1976d2; margin-top: 0;\">üì§ Upload Your Excel Files</h3>\n",
    "        <p><strong>Please upload your Excel files to this folder:</strong></p>\n",
    "        <code style=\"background: #fff; padding: 5px; border-radius: 3px; font-size: 14px;\">{INPUT_DIR}</code>\n",
    "        \n",
    "        <h4>How to Upload:</h4>\n",
    "        <ol>\n",
    "            <li>Click the <strong>üìÅ Files</strong> icon in the left sidebar</li>\n",
    "            <li>Navigate to: <code>/content/sema/data/input/</code></li>\n",
    "            <li>Click the <strong>üì§ Upload</strong> button</li>\n",
    "            <li>Select your Excel files (.xlsx)</li>\n",
    "            <li>Wait for upload to complete</li>\n",
    "            <li>Re-run this cell to verify</li>\n",
    "        </ol>\n",
    "        \n",
    "        <h4>File Requirements:</h4>\n",
    "        <ul>\n",
    "            <li>Excel format (.xlsx)</li>\n",
    "            <li>Must have <strong>VOC1</strong> column with Korean text</li>\n",
    "            <li>Optional <strong>VOC2</strong> column for additional text</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Stop execution until files are uploaded\n",
    "    raise Exception(f\"Please upload Excel files to {INPUT_DIR} first!\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nüìã Files ready for processing:\")\n",
    "    for i, file in enumerate(excel_files, 1):\n",
    "        file_size = file.stat().st_size / (1024*1024)  # MB\n",
    "        print(f\"   {i}. {file.name} ({file_size:.1f}MB)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Ready to process {len(excel_files)} files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Initialize SEMA & Process All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from src.cli import SemaInference\n",
    "\n",
    "class ClientSemaProcessor:\n",
    "    def __init__(self, model_config, input_dir, output_dir, logs_dir):\n",
    "        self.config = model_config\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.logs_dir = Path(logs_dir)\n",
    "        self.start_time = time.time()\n",
    "        self.processed_files = []\n",
    "        self.failed_files = []\n",
    "        self.sema = None\n",
    "        \n",
    "    def initialize_sema(self):\n",
    "        \"\"\"Initialize SEMA with fallback handling\"\"\"\n",
    "        print(f\"üß† Initializing SEMA {self.config['size']} model...\")\n",
    "        \n",
    "        try:\n",
    "            # Clear GPU memory first\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Initialize SEMA\n",
    "            self.sema = SemaInference(\n",
    "                model_path='team-lucid/deberta-v3-xlarge-korean' if 'xlarge' in self.config['size'] else 'team-lucid/deberta-v3-base-korean',\n",
    "                checkpoint_path=f\"model/{self.config['checkpoint']}\"\n",
    "            )\n",
    "            \n",
    "            # Set batch size\n",
    "            self.sema.batch_size = self.config['batch_size']\n",
    "            \n",
    "            print(f\"‚úÖ SEMA initialized successfully!\")\n",
    "            print(f\"   Model: {self.config['size']}\")\n",
    "            print(f\"   Batch Size: {self.sema.batch_size}\")\n",
    "            print(f\"   GPU Memory: {self.config['gpu_memory']:.1f}GB\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if 'out of memory' in error_msg or 'cuda' in error_msg:\n",
    "                print(f\"‚ùå GPU Memory Error: {e}\")\n",
    "                \n",
    "                if self.config['size'] == 'xlarge':\n",
    "                    print(\"üîÑ Attempting fallback to smaller settings...\")\n",
    "                    \n",
    "                    # Try smaller batch size first\n",
    "                    if self.config['batch_size'] > 2:\n",
    "                        self.config['batch_size'] = max(2, self.config['batch_size'] // 2)\n",
    "                        print(f\"   Reducing batch size to {self.config['batch_size']}\")\n",
    "                        return self.initialize_sema()  # Recursive retry\n",
    "                    \n",
    "                    # If still failing, suggest small model\n",
    "                    print(\"üí° XLarge model requires more memory than available\")\n",
    "                    print(\"   Please upload a small model or use a higher-tier GPU\")\n",
    "                    return False\n",
    "                else:\n",
    "                    print(\"‚ùå Even small model failed - GPU memory critically low\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\"‚ùå Unexpected initialization error: {e}\")\n",
    "                traceback.print_exc()\n",
    "                return False\n",
    "    \n",
    "    def process_single_file(self, input_file):\n",
    "        \"\"\"Process a single Excel file with timeout and error handling\"\"\"\n",
    "        file_start = time.time()\n",
    "        output_file = self.output_dir / f\"{input_file.stem}_output.xlsx\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nüìÑ Processing: {input_file.name}\")\n",
    "            \n",
    "            # Process file\n",
    "            success = self.sema.process_file(str(input_file), str(output_file))\n",
    "            \n",
    "            elapsed = time.time() - file_start\n",
    "            \n",
    "            if success and output_file.exists():\n",
    "                output_size = output_file.stat().st_size / (1024*1024)  # MB\n",
    "                print(f\"‚úÖ Completed: {input_file.name} ‚Üí {output_file.name}\")\n",
    "                print(f\"   Time: {elapsed:.1f}s, Size: {output_size:.1f}MB\")\n",
    "                self.processed_files.append(input_file.name)\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"‚ùå Failed: {input_file.name} (no output generated)\")\n",
    "                self.failed_files.append(input_file.name)\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - file_start\n",
    "            print(f\"‚ùå Error processing {input_file.name}: {e}\")\n",
    "            print(f\"   Time elapsed: {elapsed:.1f}s\")\n",
    "            \n",
    "            # Log detailed error\n",
    "            error_log = {\n",
    "                'file': input_file.name,\n",
    "                'error': str(e),\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'processing_time': elapsed,\n",
    "                'model_config': self.config,\n",
    "                'traceback': traceback.format_exc()\n",
    "            }\n",
    "            \n",
    "            error_file = self.logs_dir / 'errors' / f\"{input_file.stem}_error.json\"\n",
    "            with open(error_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(error_log, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            self.failed_files.append(input_file.name)\n",
    "            return False\n",
    "    \n",
    "    def process_all(self):\n",
    "        \"\"\"Process all Excel files in input directory\"\"\"\n",
    "        # Initialize SEMA\n",
    "        if not self.initialize_sema():\n",
    "            return False\n",
    "        \n",
    "        # Get input files\n",
    "        excel_files = list(self.input_dir.glob('*.xlsx'))\n",
    "        total_files = len(excel_files)\n",
    "        \n",
    "        if total_files == 0:\n",
    "            print(\"‚ùå No Excel files found in input directory\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting batch processing of {total_files} files...\")\n",
    "        print(f\"‚è∞ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"üß† Model: {self.config['size']} (batch_size={self.config['batch_size']})\")\n",
    "        \n",
    "        # Process each file\n",
    "        for i, excel_file in enumerate(excel_files, 1):\n",
    "            print(f\"\\nüìä Progress: {i}/{total_files}\")\n",
    "            self.process_single_file(excel_file)\n",
    "            \n",
    "            # Memory cleanup between files\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Final summary\n",
    "        total_time = time.time() - self.start_time\n",
    "        success_rate = len(self.processed_files) / total_files * 100\n",
    "        \n",
    "        print(f\"\\nüéâ Processing Complete!\")\n",
    "        print(f\"   ‚úÖ Successful: {len(self.processed_files)}/{total_files} files ({success_rate:.1f}%)\")\n",
    "        print(f\"   ‚ùå Failed: {len(self.failed_files)} files\")\n",
    "        print(f\"   ‚è∞ Total time: {total_time:.1f}s\")\n",
    "        print(f\"   üß† Model used: {self.config['size']}\")\n",
    "        \n",
    "        if self.failed_files:\n",
    "            print(f\"\\n‚ö†Ô∏è Failed files: {', '.join(self.failed_files)}\")\n",
    "            print(f\"üìÅ Error details saved in: {self.logs_dir}/errors/\")\n",
    "        \n",
    "        return len(self.processed_files) > 0\n",
    "\n",
    "# Initialize and run processor\n",
    "processor = ClientSemaProcessor(MODEL_CONFIG, INPUT_DIR, OUTPUT_DIR, LOGS_DIR)\n",
    "processing_success = processor.process_all()\n",
    "\n",
    "if processing_success:\n",
    "    print(\"\\nüåü All processing completed successfully!\")\n",
    "else:\n",
    "    print(\"\\nüí• Processing failed or incomplete\")\n",
    "    print(\"üîç Check the error logs above for details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 5: Download Results Automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Check output files\n",
    "output_files = list(OUTPUT_DIR.glob('*.xlsx'))\n",
    "total_outputs = len(output_files)\n",
    "\n",
    "print(f\"üìÅ Output files generated: {total_outputs}\")\n",
    "\n",
    "if total_outputs == 0:\n",
    "    print(\"‚ùå No output files to download\")\n",
    "    print(\"üîç Check processing errors above\")\n",
    "    \n",
    "elif total_outputs == 1:\n",
    "    # Single file - download directly\n",
    "    output_file = output_files[0]\n",
    "    print(f\"üì• Downloading: {output_file.name}\")\n",
    "    try:\n",
    "        files.download(str(output_file))\n",
    "        print(f\"‚úÖ Downloaded: {output_file.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    # Multiple files - create zip\n",
    "    zip_filename = f\"SEMA_Results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
    "    zip_path = OUTPUT_DIR / zip_filename\n",
    "    \n",
    "    print(f\"üì¶ Creating zip file: {zip_filename}\")\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for output_file in output_files:\n",
    "                zipf.write(output_file, output_file.name)\n",
    "                print(f\"   ‚úÖ Added: {output_file.name}\")\n",
    "        \n",
    "        # Download zip file\n",
    "        zip_size = zip_path.stat().st_size / (1024*1024)  # MB\n",
    "        print(f\"\\nüì• Downloading zip file ({zip_size:.1f}MB)...\")\n",
    "        files.download(str(zip_path))\n",
    "        print(f\"‚úÖ Downloaded: {zip_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Zip creation/download failed: {e}\")\n",
    "        print(\"üì• Downloading individual files...\")\n",
    "        \n",
    "        # Fallback to individual downloads\n",
    "        for output_file in output_files:\n",
    "            try:\n",
    "                files.download(str(output_file))\n",
    "                print(f\"‚úÖ Downloaded: {output_file.name}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Failed to download {output_file.name}: {e2}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nüèÅ SEMA Client Complete - Session Summary:\")\n",
    "print(f\"   üìä Input files: {len(excel_files)}\")\n",
    "print(f\"   ‚úÖ Processed: {len(processor.processed_files)}\")\n",
    "print(f\"   üì• Downloaded: {total_outputs}\")\n",
    "print(f\"   üß† Model: {MODEL_CONFIG['size']}\")\n",
    "print(f\"   üíæ Cache: {'Drive enabled' if USE_DRIVE_CACHE else 'Direct download'}\")\n",
    "\n",
    "if len(processor.failed_files) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è Failed: {len(processor.failed_files)} files\")\n",
    "    print(f\"   üìÅ Error logs: {LOGS_DIR}/errors/\")\n",
    "\n",
    "print(f\"\\nüéä All done! Check your Downloads folder for results.\")\n",
    "\n",
    "# Show output format info\n",
    "if total_outputs > 0:\n",
    "    print(f\"\\nüìã Output File Format:\")\n",
    "    print(f\"   ‚Ä¢ Original columns preserved\")\n",
    "    print(f\"   ‚Ä¢ VOC: Processed Korean text\")\n",
    "    print(f\"   ‚Ä¢ topic: Classified topics\")\n",
    "    print(f\"   ‚Ä¢ sentiment: Sentiment analysis\")\n",
    "    print(f\"   ‚Ä¢ keyword: Extracted keywords\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}